# Commlink

## Purpose

Commlink is a **platform for training autonomous drones using a two-level AI control system**:
- **High-Level Planner (LLM)**: Understands natural language objectives, reasons about the world, and generates plans
- **Low-Level Controller (World Model)**: Predicts physics and executes precise flight maneuvers

## Target Workflow

1. **Define Scenario** - Natural language objective ("Locate and track the red sphere") + exit criteria
2. **Run Variants** - Execute multiple simulation runs with randomized parameters
3. **Collect Data** - Capture video, telemetry, LLM decisions, and controller actions
4. **Train Models** - Improve vision, planning, and control models from collected data
5. **Deploy & Iterate** - Test new models, compare performance, repeat

### Two-Level AI Control

```
Natural Language Objective
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HIGH-LEVEL PLANNER    â”‚  â† LLM (Claude/GPT/Llama)
â”‚   (LLM)                 â”‚
â”‚                         â”‚
â”‚   "Locate red sphere"   â”‚
â”‚   â†’ ["search", "track"] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ subgoals
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LOW-LEVEL CONTROLLER  â”‚  â† World Model (DreamerV3)
â”‚   (World Model + MPC)   â”‚
â”‚                         â”‚
â”‚   subgoal + state       â”‚
â”‚   â†’ motor commands      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What is a World Model?

A world model is a neural network that learns to predict how the environment will change in response to actions. For Commlink:

- **Input**: Current drone state (position, velocity, orientation) + camera image + action
- **Output**: Predicted future trajectory (N-step position predictions)
- **Architecture**: DreamerV3-inspired latent dynamics model
- **Use Case**: Model Predictive Control (MPC) for precise flight maneuvers

## Documentation

- **[Architecture](docs/ARCHITECTURE.md)** - Full system design with diagrams
- **[Implementation Plan](docs/IMPLEMENTATION_PLAN.md)** - Detailed roadmap to target architecture

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VERCEL FRONTEND (Next.js)                       â”‚
â”‚              Dashboard, Runs, Scenarios, Models pages               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            SUPABASE                                 â”‚
â”‚        PostgreSQL (runs, metrics, models, scenarios, episodes)      â”‚
â”‚        Storage (episode data), Realtime (live updates)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                    â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LOCAL SIMULATION       â”‚              â”‚   LOCAL TRAINING         â”‚
â”‚   (Docker)               â”‚              â”‚   (RTX 5070 GPU)         â”‚
â”‚                          â”‚              â”‚                          â”‚
â”‚   PX4 SITL + Gazebo      â”‚              â”‚   PyTorch World Model    â”‚
â”‚   MAVLink (pymavlink)    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   DreamerV3 architecture â”‚
â”‚   Data Collector         â”‚   episodes   â”‚   Trajectory prediction  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Tech Stack

| Layer | Technology |
|-------|------------|
| Frontend | Next.js 15, TypeScript, Tailwind CSS |
| Backend | Supabase (PostgreSQL, Storage, Realtime) |
| Hosting | Vercel (auto-deploy from GitHub) |
| Simulation | PX4 SITL, Gazebo, MAVLink (MAVSDK/pymavlink) |
| ML Training | PyTorch, DreamerV3-style architecture |
| Local GPU | RTX 5070 (or any CUDA-capable GPU) |
| Dev Environment | GitHub Codespaces |

## Project Structure

```
commlink/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx                    # Dashboard - stats overview
â”‚   â”‚   â”œâ”€â”€ layout.tsx                  # Root layout with sidebar
â”‚   â”‚   â”œâ”€â”€ runs/
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx                # Training runs list
â”‚   â”‚   â”‚   â”œâ”€â”€ NewRunButton.tsx        # Create run modal
â”‚   â”‚   â”‚   â””â”€â”€ [id]/
â”‚   â”‚   â”‚       â”œâ”€â”€ page.tsx            # Run detail
â”‚   â”‚   â”‚       â”œâ”€â”€ RunActions.tsx      # Status actions
â”‚   â”‚   â”‚       â””â”€â”€ MetricsChart.tsx    # Loss/MSE chart
â”‚   â”‚   â”œâ”€â”€ scenarios/
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx                # Scenarios list
â”‚   â”‚   â”‚   â””â”€â”€ NewScenarioButton.tsx   # Create scenario modal
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx                # Models registry
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ runs/[id]/
â”‚   â”‚           â””â”€â”€ start-training/     # API to trigger training
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ Sidebar.tsx                 # Navigation sidebar
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ supabase/                   # Supabase clients
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ index.ts                    # TypeScript interfaces
â”œâ”€â”€ simulation/
â”‚   â”œâ”€â”€ Dockerfile                      # PX4 SITL + Gazebo container
â”‚   â”œâ”€â”€ docker-compose.yml              # Orchestration
â”‚   â”œâ”€â”€ agent.py                        # MAVLink agent for data collection
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ model.py                        # World model (PyTorch)
â”‚   â”œâ”€â”€ train.py                        # Training script
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_local.py                    # Local runner (orchestrates everything)
â””â”€â”€ supabase/
    â””â”€â”€ schema.sql                      # Database schema
```

## Implementation Status

### Phase 1: Foundation âœ… COMPLETE
- [x] Dashboard UI (Next.js, TypeScript, Tailwind)
- [x] Database schema (Supabase)
- [x] World model architecture (DreamerV3-style)
- [x] Progress tracking with ETA
- [x] Demo mode for easy onboarding

### Phase 2: Perception ğŸ”² PLANNED
- [ ] Camera integration in Gazebo
- [ ] Object detection (YOLOv8)
- [ ] Video recording
- [ ] Detection logging

### Phase 3: LLM Planner ğŸ”² PLANNED
- [ ] Natural language objectives
- [ ] LLM integration (Claude API)
- [ ] Decision logging
- [ ] Subgoal generation

### Phase 4: Autonomous Control ğŸ”² PLANNED
- [ ] Mid-level planner (subgoal â†’ waypoint)
- [ ] MPC controller (world model integration)
- [ ] Exit condition handling
- [ ] Variant generation

### Phase 5: Training Pipeline ğŸ”² PLANNED
- [ ] Vision model training
- [ ] LLM fine-tuning pipeline
- [ ] Enhanced world model training

### Phase 6: Analytics ğŸ”² PLANNED
- [ ] Performance analytics dashboard
- [ ] A/B testing framework
- [ ] Episode replay viewer

See [IMPLEMENTATION_PLAN.md](docs/IMPLEMENTATION_PLAN.md) for detailed breakdown.

## Quick Start

### 1. Run the Demo (No GPU Required)

Test the training pipeline with dummy data:

```bash
# Install Python dependencies
pip install torch numpy supabase

# Run demo
python scripts/run_local.py --demo
```

### 2. Run with Supabase Integration

```bash
# Set environment variables
export NEXT_PUBLIC_SUPABASE_URL=https://lwkmwwvzpzgeehqtnuht.supabase.co
export NEXT_PUBLIC_SUPABASE_ANON_KEY=your-key

# Create a run in the dashboard, then:
python scripts/run_local.py --run-id <your-run-id>
```

### 3. Run Full Simulation (Docker)

```bash
cd simulation
docker-compose up
```

## Database Schema

```sql
runs (id, name, status, config, created_at, updated_at)
  â””â”€â”€ status: pending â†’ collecting â†’ training â†’ evaluating â†’ completed

metrics (id, run_id, epoch, loss, trajectory_mse, timestamp)

models (id, run_id, version, checkpoint_url, eval_score, created_at)

scenarios (id, name, environment, waypoints, duration, config)

episodes (id, run_id, scenario_id, data_url, frames, created_at)
```

## World Model Architecture

```
Input: (state_t, action_t)
   â”‚
   â”œâ”€â–º StateEncoder (MLP) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                                   â”‚
   â””â”€â–º ActionEncoder (MLP) â”€â”€â”         â”‚
                             â”‚         â”‚
                             â–¼         â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ DynamicsModel   â”‚
                        â”‚ (GRU + MLP)     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ TrajectoryDecoderâ”‚
                        â”‚ (MLP â†’ xyz)     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                        predicted_position[t+1:t+N]
```

## Commands

### Frontend
```bash
npm run dev      # Start Next.js dev server
npm run build    # Production build
npm run lint     # ESLint
```

### Training
```bash
# Demo mode (dummy data)
python scripts/run_local.py --demo

# With Supabase run
python scripts/run_local.py --run-id <uuid>

# Direct training script
python training/train.py --use-dummy-data --epochs 50
```

### Simulation
```bash
cd simulation
docker-compose up          # Start PX4 + Gazebo + Agent
docker-compose down        # Stop all
```

## URLs

- **Production**: https://commlink-ew2rckdr7-fsilva7456s-projects.vercel.app
- **GitHub**: https://github.com/fsilva7456/commlink
- **Supabase**: https://supabase.com/dashboard/project/lwkmwwvzpzgeehqtnuht

## Development Workflow

```bash
# 1. Create feature branch
git checkout master && git pull
git checkout -b feature/your-feature

# 2. Make changes and test
npm run dev
python scripts/run_local.py --demo

# 3. Commit and push
git add -A
git commit -m "Description"
git push -u origin feature/your-feature

# 4. Create PR
gh pr create --title "Title" --body "Description"
```
